\documentclass{article}

% Reduce margins
\usepackage[a4paper, margin=1in]{geometry}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{hyperref}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setcounter{secnumdepth}{0} % remove section numbering

\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>0.6\linewidth 0.6\linewidth \else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

\author{}
\date{}

\begin{document}

\textbf{\uline{AI -- Assignment 2}}

\textbf{SemEval-2026 Task 13: Detecting Machine-Generated Code with Multiple Programming Languages, Generators, and Application~Scenarios}

\textbf{Domain:} Subtask A -- Binary Machine-Generated Code Detection

\textbf{GitHub Repository:} \url{https://github.com/SagaciousPluto/AI-Assignment-2}

\begin{longtable}{p{0.33\linewidth} p{0.33\linewidth} p{0.33\linewidth}}
\toprule
\textbf{Name} & \textbf{CMS} & \textbf{Model} \\
\midrule
\endhead
M. Ramis Khan & 504650 & Model\_A \\
M. Moazzam Ali & 509740 & Model\_B \\
Muhammad Abdullah & 503871 & Model\_C \\
\bottomrule
\end{longtable}

\textbf{Baseline Architectures:}

\textbf{Model A Baseline:} XGB Classifier -- Optimized open-source model that utilizes decision trees.

\textbf{Model B Baseline:} CodeBERT -- Pretrained Bert model customized for code-based datasets.

\textbf{Model C Baseline:} DistilBERT -- Faster and more efficient version of BERT model for NLP.

\textbf{Model A:}

\textbf{Pipeline:}

\includegraphics[width=0.6\linewidth]{plots/model_A_pipeline.png}

\textbf{Hyperparameters:}

\begin{itemize}
\item N\_ESTIMATORS=300
\item MAX\_DEPTH=5
\item LEARNING\_RATE=0.1
\item TREE\_METHOD='GPU\_HIST'
\item PREDICTOR='GPU\_PREDICTOR'
\item EVAL\_METRIC='LOGLOSS'
\end{itemize}

\textbf{Validation Accuracy:} 94.45\%

\textbf{Test Accuracy:} 29.90\%

\textbf{Model B:}

\textbf{Pipeline:}

\includegraphics[width=0.6\linewidth]{plots/model_B_pipeline.png}

\textbf{Hyperparameters:}

\begin{itemize}
\item LR = 2e-5
\item BATCH\_SIZE = 8
\item EPOCHS= 3
\item WARMUP\_STEPS = 0
\item MAX\_LENGTH = 256
\item DROPOUT = 0.3
\item MODEL\_NAME = "microsoft/codebert-base"
\item DEVICE = "cuda"
\end{itemize}

\textbf{Validation Accuracy:} 98.68\%

\textbf{Testing Accuracy:} 35.60\%

\textbf{Model C:}

\textbf{Pipeline:}

\includegraphics[width=0.6\linewidth]{plots/model_C_pipeline.png}

\textbf{Hyperparameters:}

\begin{itemize}
\item EPOCHS = 2
\item LEARNING\_RATE = 2E-5
\item OPTIMIZER = ADAMW
\item WEIGHT\_DECAY = 0.01
\item BATCH\_SIZE = 32
\item GRAD\_ACC = 4
\item SCHEDULER = LINEAR WARMUP
\item LOSS\_FN = CROSSENTROPYLOSS WITH CLASS WEIGHTS
\end{itemize}

\textbf{Validation Accuracy:} 96.04\%

\textbf{Test Accuracy:} 29.10\%

\textbf{Results:}

\textbf{Validation:}

\begin{longtable}{p{0.25\linewidth} p{0.25\linewidth} p{0.25\linewidth} p{0.25\linewidth}}
\toprule
\textbf{Models} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} \\
\midrule
\endhead
Model\_A & 94.45 & 94.44 & 94.45 \\
Model\_B & 98.68 & 98.91 & 98.55 \\
Model\_C & 96.04 & 96.10 & 96.04 \\
\bottomrule
\end{longtable}

\textbf{Test:}

\begin{longtable}{p{0.25\linewidth} p{0.25\linewidth} p{0.25\linewidth} p{0.25\linewidth}}
\toprule
\textbf{Models} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} \\
\midrule
\endhead
Model\_A & 29.90 & 78.50 & 29.90 \\
Model\_B & 35.60 & 24.36 & 89.69 \\
Model\_C & 29.10 & 64.77 & 29.10 \\
\bottomrule
\end{longtable}

\textbf{Implementation Issues:}

\begin{enumerate}
\item Ram usage frequently exceeded the limit which had to be optimized.
\item Model training took a lot of time so we had to limit the number of epochs.
\end{enumerate}

\end{document}
